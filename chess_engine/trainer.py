from typing import Optional, List, Tuple
import torch
import chess
from .model import ValueNet, PolicyNet
from .dataset import ChessDataset
import torch.nn as nn


class Trainer:
    """Trainer that supports self-play unsupervised training for policy+value.

    Approach:
    - Policy scores resulting states for each legal move; softmax over scores
      yields a distribution over moves.
    - Self-play games are generated by sampling moves from the policy.
    - After each batch of games we compute returns (game result) and update:
        * ValueNet via MSE to the return (from the perspective of the player to move at that state)
        * PolicyNet via REINFORCE using advantage = return - value(state)
    This is a minimal, extensible setup for experimentation.
    """

    def __init__(self, policy: Optional[PolicyNet] = None, value: Optional[ValueNet] = None, device: Optional[str] = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = policy or PolicyNet()
        self.value = value or ValueNet()
        self.policy.to(self.device)
        self.value.to(self.device)

    def self_play_episode(self, temperature: float = 1.0, max_moves: int = 20) -> Tuple[List[Tuple[torch.Tensor, torch.Tensor, bool, chess.Move]], float, dict]:
        """Generate one self-play game.

        Returns:
            traj: list of tuples (state_tensor, log_prob, is_white_to_move, move)
            outcome: +1 for white win, -1 for black win, 0 for draw
            info: dict with keys `length`, `captures`, `material_swing`
        """
        import chess
        from .utils import board_to_tensor, select_move_and_logprob

        board = chess.Board()
        traj = []
        moves_made = 0
        captures = 0
        while not board.is_game_over() and moves_made < max_moves:
            state_tensor = board_to_tensor(board).to(self.device)
            chosen, logp, moves, probs = select_move_and_logprob(self.policy, board, temperature=temperature, device=self.device)
            if chosen is None:
                break
            is_white = board.turn  # True for white to move
            # Count captures (board.is_capture uses the pre-move board)
            if board.is_capture(chosen):
                captures += 1
            traj.append((state_tensor, logp, is_white, chosen))
            board.push(chosen)
            moves_made += 1

        # If the game ended normally, use the official result. If we exited
        # because we hit `max_moves` (board not game-over), decide the result
        # based on material advantage (white win / black win / draw). We also
        # record whether the decision was made by material (i.e. max moves hit)
        # so the training loop can report statistics.
        decided_by_material = not board.is_game_over()
        if board.is_game_over():
            result = board.result(claim_draw=True)
            if result == "1-0":
                z = 1.0
            elif result == "0-1":
                z = -1.0
            else:
                z = 0.0
        else:
            # Reached max moves; decide by material balance
            white_mat = 0.0
            black_mat = 0.0
            values = {
                chess.PAWN: 1.0,
                chess.KNIGHT: 3.0,
                chess.BISHOP: 3.0,
                chess.ROOK: 5.0,
                chess.QUEEN: 9.0,
                chess.KING: 0.0,
            }
            for _, piece in board.piece_map().items():
                val = values.get(piece.piece_type, 0.0)
                if piece.color:
                    white_mat += val
                else:
                    black_mat += val

            if white_mat - black_mat > 1.0:
                z = 1.0
            elif black_mat - white_mat > 1.0:
                z = -1.0
            else:
                z = 0.0
        # Compute material swing (absolute difference white - black)
        white_mat = 0.0
        black_mat = 0.0
        values = {
            chess.PAWN: 1.0,
            chess.KNIGHT: 3.0,
            chess.BISHOP: 3.0,
            chess.ROOK: 5.0,
            chess.QUEEN: 9.0,
            chess.KING: 0.0,
        }
        for sq, piece in board.piece_map().items():
            val = values.get(piece.piece_type, 0.0)
            if piece.color:
                white_mat += val
            else:
                black_mat += val

        material_swing = abs(white_mat - black_mat)
        info = {"length": moves_made, "captures": captures, "material_swing": material_swing, "decided_by_material": decided_by_material, "finished": board.is_game_over()}
        return traj, z, info

    def train_self_play(self, iterations: int = 100, games_per_iter: int = 8, lr_policy: float = 1e-3, lr_value: float = 1e-3, temperature: float = 1.0, illegal_penalty: float = -10.0, last_k: Optional[int] = None):
        """Run repeated self-play and training cycles.

        This function collects `games_per_iter` games each iteration, then:
        - Updates ValueNet to predict discounted cumulative returns (game outcomes with time decay)
        - Updates PolicyNet to predict which destination squares lead to high-value positions,
          with severe penalties for illegal moves.
        
        Args:
            last_k: if >0, only keep the final `last_k` states from each self-play game
                for training (focuses learning on late-game signals).
        """
        import chess
        from .utils import board_to_tensor

        policy_opt = torch.optim.Adam(self.policy.parameters(), lr=lr_policy)
        value_opt = torch.optim.Adam(self.value.parameters(), lr=lr_value)
        mse = nn.MSELoss()

        for it in range(1, iterations + 1):
            all_states = []
            all_returns = []
            all_policy_targets = []  # (batch, 768) targets for policy
            outcomes = {"white_wins": 0, "black_wins": 0, "draws": 0}

            # accumulate per-iteration stats
            total_length = 0
            total_captures = 0
            total_material_swing = 0.0
            finished_games = 0
            material_decided_games = 0
            material_wins_by_material = 0

            for g in range(games_per_iter):
                traj, z, info = self.self_play_episode(temperature=temperature)
                total_length += info.get("length", 0)
                total_captures += info.get("captures", 0)
                total_material_swing += info.get("material_swing", 0.0)
                # finished normally (game-over) vs decided by material at max_moves
                if info.get("finished"):
                    finished_games += 1
                if info.get("decided_by_material"):
                    material_decided_games += 1
                    if z != 0.0:
                        material_wins_by_material += 1
                # Track outcome
                if z == 1.0:
                    outcomes["white_wins"] += 1
                elif z == -1.0:
                    outcomes["black_wins"] += 1
                else:
                    outcomes["draws"] += 1

                results = [z for _, _, _, _ in traj]
                # Optionally focus training on the final `last_k` states of the game
                if last_k is not None and last_k > 0 and len(traj) > last_k:
                    traj = traj[-last_k:]
                    results = results[-last_k:]

                # Reconstruct game moves
                board = chess.Board()
                for (state, logp, is_white, move), res in zip(traj, results):
                    all_states.append(state)
                    all_returns.append(torch.tensor(res, dtype=torch.float32, device=self.device))

                    # Build policy target: 4096-dim vector with penalties for illegal, values for legal
                    policy_target = torch.full((4096,), illegal_penalty, dtype=torch.float32, device=self.device)

                    # Get legal moves and evaluate their resulting states.
                    # If a resulting state is terminal, override the value with
                    # the forced game outcome (+1 white win, -1 black win, 0 draw)
                    current_player_is_white = board.turn
                    legal_moves = list(board.legal_moves)
                    next_boards = []
                    nonterminal_move_idxs = []  # list of indices for non-terminal next states
                    for mv in legal_moves:
                        b2 = board.copy()
                        b2.push(mv)
                        from_sq = mv.from_square
                        to_sq = mv.to_square
                        idx = from_sq * 64 + to_sq
                        if b2.is_game_over():
                            res = b2.result(claim_draw=True)
                            if res == "1-0":
                                z2 = 1.0
                            elif res == "0-1":
                                z2 = -1.0
                            else:
                                z2 = 0.0
                            # z2 is white-perspective result of the terminal next state.
                            # Convert to the current player's perspective (player making the move)
                            val = z2 if current_player_is_white else -z2
                            policy_target[idx] = val
                        else:
                            next_boards.append(board_to_tensor(b2).to(self.device))
                            nonterminal_move_idxs.append(idx)

                    # Evaluate non-terminal next states with the value network (detached for target)
                    if len(next_boards) > 0:
                        next_states = torch.stack(next_boards)
                        with torch.no_grad():
                            next_values = self.value(next_states)
                            next_values = next_values.view(-1)
                        # Assign values to the corresponding move indices (vectorized).
                        # `next_values` is a tensor on `self.device` with one value per
                        # non-terminal next-state; build an index tensor and assign
                        # all values at once to avoid repeated .item() calls.
                        idx_tensor = torch.tensor(nonterminal_move_idxs, dtype=torch.long, device=self.device)
                        vals = next_values if current_player_is_white else -next_values
                        policy_target[idx_tensor] = vals

                    all_policy_targets.append(policy_target)
                    board.push(move)

            if len(all_states) == 0:
                print(f"Iter {it}: no training data collected")
                continue

            states = torch.stack(all_states)
            returns = torch.stack(all_returns)
            policy_targets = torch.stack(all_policy_targets)

            # Update value network
            self.value.train()
            value_opt.zero_grad()
            preds = self.value(states)
            value_loss = mse(preds, returns)
            value_loss.backward()
            value_opt.step()

            # Update policy network: learn to score legal moves by their resulting value
            self.policy.train()
            policy_opt.zero_grad()
            policy_preds = self.policy(states)  # (batch, 768)
            policy_loss = mse(policy_preds, policy_targets)
            policy_loss.backward()
            policy_opt.step()

            # Compute outcome percentages
            total_games = sum(outcomes.values())
            if total_games > 0:
                white_pct = 100.0 * outcomes["white_wins"] / total_games
                black_pct = 100.0 * outcomes["black_wins"] / total_games
                draw_pct = 100.0 * outcomes["draws"] / total_games
            else:
                white_pct = black_pct = draw_pct = 0.0

            # Compute average game stats
            games_counted = games_per_iter if games_per_iter > 0 else 1
            avg_len = total_length / games_counted
            avg_caps = total_captures / games_counted
            avg_mat = total_material_swing / games_counted

            material_decided_pct = 100.0 * material_decided_games / games_counted
            material_wins_by_material_pct = 100.0 * material_wins_by_material / games_counted

            print(f"Iter {it}/{iterations}: games={games_per_iter} finished={finished_games}/{games_per_iter} states={len(all_states)} value_loss={value_loss.item():.4f} policy_loss={policy_loss.item():.4f} | W:{white_pct:.1f}% B:{black_pct:.1f}% D:{draw_pct:.1f}% | material_decided={material_decided_pct:.1f}% material_wins_by_material={material_wins_by_material_pct:.1f}% | avg_len={avg_len:.1f} avg_caps={avg_caps:.2f} avg_mat_swing={avg_mat:.2f}")

    def save(self, prefix: str):
        torch.save(self.policy.state_dict(), prefix + "_policy.pt")
        torch.save(self.value.state_dict(), prefix + "_value.pt")

    def load(self, prefix: str):
        self.policy.load_state_dict(torch.load(prefix + "_policy.pt", map_location=self.device))
        self.value.load_state_dict(torch.load(prefix + "_value.pt", map_location=self.device))
