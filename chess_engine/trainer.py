from typing import Optional, List, Tuple
import torch
from .model import ValueNet, PolicyNet
from .dataset import ChessDataset
import torch.nn as nn


class Trainer:
    """Trainer that supports self-play unsupervised training for policy+value.

    Approach:
    - Policy scores resulting states for each legal move; softmax over scores
      yields a distribution over moves.
    - Self-play games are generated by sampling moves from the policy.
    - After each batch of games we compute returns (game result) and update:
        * ValueNet via MSE to the return (from the perspective of the player to move at that state)
        * PolicyNet via REINFORCE using advantage = return - value(state)
    This is a minimal, extensible setup for experimentation.
    """

    def __init__(self, policy: Optional[PolicyNet] = None, value: Optional[ValueNet] = None, device: Optional[str] = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = policy or PolicyNet()
        self.value = value or ValueNet()
        self.policy.to(self.device)
        self.value.to(self.device)

    def self_play_episode(self, temperature: float = 1.0, max_moves: int = 512) -> Tuple[List[Tuple[torch.Tensor, torch.Tensor, bool]], float]:
        """Generate one self-play game.

        Returns:
            traj: list of tuples (state_tensor, log_prob, is_white_to_move)
            outcome: +1 for white win, -1 for black win, 0 for draw
        """
        import chess
        from .utils import board_to_tensor, select_move_and_logprob

        board = chess.Board()
        traj = []
        moves_made = 0
        while not board.is_game_over() and moves_made < max_moves:
            state_tensor = board_to_tensor(board).to(self.device)
            chosen, logp, moves, probs = select_move_and_logprob(self.policy, board, temperature=temperature, device=self.device)
            if chosen is None:
                break
            is_white = board.turn  # True for white to move
            traj.append((state_tensor, logp, is_white))
            board.push(chosen)
            moves_made += 1

        result = board.result(claim_draw=True)
        if result == "1-0":
            z = 1.0
        elif result == "0-1":
            z = -1.0
        else:
            z = 0.0
        return traj, z

    def train_self_play(self, iterations: int = 100, games_per_iter: int = 8, lr_policy: float = 1e-4, lr_value: float = 1e-3, temperature: float = 1.0):
        """Run repeated self-play and training cycles.

        This function collects `games_per_iter` games each iteration, then
        updates the policy and value networks on the collected trajectories.
        """
        policy_opt = torch.optim.Adam(self.policy.parameters(), lr=lr_policy)
        value_opt = torch.optim.Adam(self.value.parameters(), lr=lr_value)
        mse = nn.MSELoss()

        for it in range(1, iterations + 1):
            all_states = []
            all_logps = []
            all_returns = []

            for g in range(games_per_iter):
                traj, z = self.self_play_episode(temperature=temperature)
                for state, logp, is_white in traj:
                    # Return from the perspective of the player to move at that state
                    ret = z if is_white else -z
                    all_states.append(state)
                    all_logps.append(logp)
                    all_returns.append(torch.tensor(ret, dtype=torch.float32, device=self.device))

            if len(all_states) == 0:
                print(f"Iter {it}: no training data collected")
                continue

            states = torch.stack(all_states)
            returns = torch.stack(all_returns)
            logps = torch.stack(all_logps)

            # Update value network
            self.value.train()
            value_opt.zero_grad()
            preds = self.value(states)
            value_loss = mse(preds, returns)
            value_loss.backward()
            value_opt.step()

            # Update policy network with REINFORCE using value as baseline
            self.policy.train()
            policy_opt.zero_grad()
            with torch.no_grad():
                baseline = self.value(states)
            advantages = returns - baseline
            policy_loss = -(logps * advantages).mean()
            policy_loss.backward()
            policy_opt.step()

            print(f"Iter {it}/{iterations}: games={games_per_iter} states={len(all_states)} value_loss={value_loss.item():.4f} policy_loss={policy_loss.item():.4f}")

    def save(self, prefix: str):
        torch.save(self.policy.state_dict(), prefix + "_policy.pt")
        torch.save(self.value.state_dict(), prefix + "_value.pt")

    def load(self, prefix: str):
        self.policy.load_state_dict(torch.load(prefix + "_policy.pt", map_location=self.device))
        self.value.load_state_dict(torch.load(prefix + "_value.pt", map_location=self.device))
